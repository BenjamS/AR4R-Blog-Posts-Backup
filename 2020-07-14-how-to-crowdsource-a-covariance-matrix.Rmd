---
title: How to crowdsource a covariance matrix
author: Ben Schiek
date: '2020-07-14'
slug: how-to-crowdsource-a-covariance-matrix
categories:
  - Methodology
tags: []
biliography: How to crowdsource a covariance matrix.bib
smart: true
---

<style>
p.caption {
  font-size: 0.7em;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
#devtools::install_github("thomasp85/patchwork")
library(plyr)
library(tidyverse)
library(zoo)
library(tidyquant)
library(patchwork)
library(kableExtra)

this_folder <- "Data/"
```

```{r, fig.width=14, fig.height=14, fig.align='center', echo = FALSE, include = FALSE}
#options(warn = -1); options(scipen = 999)
spy_sector_symbs <- c("XLF", "XLC", "XLY", "XLP", "XLV", "XLK", "RWR",
                      "XLU", "XLI", "XBI", "IYT") #"TTEK"
spy_sector_detail <- c("Financials", "Communications",
                       "Luxury goods", "Consumer\ngoods",
                       "Healthcare", "Technology", "Real estate",
                       "Utilities", "Industrial",
                       "Biotechnology", "Transportation") #"Gov. foreign aid"
# minerals_symbs <- c("GLD", "SLV", "PPLT", "JJC", "JJM") #"XME"
# minerals_detail <- c("Gold", "Silver", "Platinum", "Copper", "Industrial metals") #"US metals and mining"
# agriculture_symbs <- c("JJG", "BAL", "SOYB", "SGG", "JO", "NIB", "DBA")
# agriculture_detail <- c("Grains", "Cotton", "Soybean", "Sugar", "Coffee", "Cacao", "General agriculture")
# energy_symbs <- c("WTI", "FUE", "WOOD", "ICLN", "KOL", "UNG")
# energy_detail <- c("Oil (W&T Offshore Inc.)", "Biofuels", "Timber", "Clean energy", "Coal", "US natural gas")
# currency_symbs <- c("EMLC", "UUP", "FXE", "FXY", "FXF", "FXC", "FXB", "FXA")
# currency_detail <- c("Emerging mkt currencies", "USD", "EUR", "JPY", "CHF", "CND", "GBP", "AUD")
currency_symbs <- c("EURUSD=X", "JPY=X", "CHF=X", "CAD=X",
                    "GBPUSD=X", "AUDUSD=X", "INR=X")
currency_detail <- c("EUR/USD", "USD/JPY",
                     "USD/CHF", "USD/CAD", "GBP/USD", "AUD/USD", "USD/INR")
# emerg_mkt_symbs <- c("EMLC", "ELD", "BKF", "VWOB")
# emerg_mkt_detail <- c("Emerg mkts debt", "Emerg mkts gov. bonds", "BRIC countries", "Emerging mkt currencies")
# crypto_symbs <- c("BLOK", "LEGR", "BCNA")
# crypto_detail <- c("Blockchain tech.", "Blockchain companies", "Blockchain in China")
# Tbond_symbs <- c("IEI", "IEF", "TLT")#, "BIL"
# Tbond_detail <- c("T-bond 3-7 yrs", "T-bond 7-10 yrs", "T-bond 20+ yrs") #"T-bond 1-3 months"
# 
# ts_symb_vec <- c(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                  currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
# ts_detail_vec <- c(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
#                    currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
ts_symb_vec <- c(spy_sector_symbs)
ts_detail_vec <- c(spy_sector_detail)
df_symb_detail <- data.frame(symbol = ts_symb_vec, detail = ts_detail_vec)
#--
#length(ts_symb_vec)
#--
# list_groups <- list(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                      currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
#-------------------------------------------
# Create grouping info for plots (required input into functions)
# list_groups <- list(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
#                     currency_detail, emerg_mkt_detail, Tbond_detail) #crypto_detail,
# group_names <- c("US Sectors", "Minerals", "Agriculture", "Energy", "Major Currency Pairs",
#                  "Emerging Markets", "T-Bonds") #"Cryptocurrencies/\nBlockchain"
# n_groups <- length(list_groups)
# #group_colors <- RColorBrewer::brewer.pal(n = n_groups, "Dark2")
# # "Darjeeling"
# # group_colors <- wesanderson::wes_palette("Darjeeling1", n = n_groups, type = "continuous")
# bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
# group_colors <- sample(bag_of_colors, n_groups)
#--------------------------------------------------------------
# Define function to order data by group
group_fn <- function(group_info){
  list_groups <- group_info[[1]]
  group_names <- group_info[[2]]
  group_colors <- group_info[[3]]
  varNames_ordered <- do.call(c, list_groups)
  n_groups <- length(group_names)
  n_items <- length(varNames_ordered)
  if(is.na(group_colors)){
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
    group_colors <- sample(bag_of_colors, n_groups)
    #group_colors <- viridis::viridis_pal(option = "D")(length(group_names))
  }
  #if(reverse_order){group_colors <- rev(group_colors)}
  #varNames_ordered <- colnames(mat_pctDiff)
  group_vec <- rep(NA, n_items)
  group_color_vec <- rep(NA, n_items)
  for(i in 1:n_groups){
    this_group_vec <- list_groups[[i]]
    this_group_name <- group_names[i]
    this_group_color <- group_colors[i]
    group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    group_color_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_color
  }
  ind_ordered_cols <- order(factor(group_vec))
  cols_ordered_by_group <- as.character(varNames_ordered[ind_ordered_cols])
  group_color_vec <- group_color_vec[ind_ordered_cols]
  group_vec_ordered <- group_vec[ind_ordered_cols]
  out_list <- list(cols_ordered_by_group, group_color_vec, group_vec_ordered, ind_ordered_cols, group_vec)
  return(out_list)
}
#--------------------------------------------------------------
# group_info <- list(list_groups, group_names, group_colors)
# outlist <- group_fn(group_info)
# cols_ordered_by_group <- outlist[[1]]
# group_color_vec <- outlist[[2]]
# group_vec_ordered <- outlist[[3]]
# ind_ordered_cols <- outlist[[4]]
# df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
#--------------------------------------------------------------
# fromdate = "2018-06-20"
# todate = "2019-08-09"
# tbl_ohlcv <- tq_get(ts_symb_vec, get = "stock.prices", from = fromdate, to = todate)
#df_ohlcv <- as.data.frame(tbl_ohlcv)
#write.csv(df_ohlcv, "Financial data backup.csv")
this_file <- "Financial data backup.csv"
this_filepath <- paste0(this_folder, this_file)
df_ohlcv <- read.csv(this_filepath, stringsAsFactors = F)
df_ohlcv <- subset(df_ohlcv, symbol %in% ts_symb_vec)
#--
# Crypto ts can have duplicate date entries sometimes (BTC-USD)
# Get rid of these duplicates, if any
# df_ohlcv <- as.data.frame(df_ohlcv %>% group_by(symbol) %>% mutate(dup = duplicated(date)))
# df_ohlcv <- subset(df_ohlcv, dup == F)
# df_ohlcv$dup <- NULL
#--

#df_ohlcv$p <- rowSums(df_ohlcv[, c(4:6)]) / 3
#--
#df <- df_ohlcv[, c("date", "symbol", "p")]
# df <- df_ohlcv %>% group_by(symbol) %>% tq_transmute(select = adjusted, 
#                      mutate_fun = periodReturn, 
#                      period     = "monthly")
#--
df <- df_ohlcv[, c("date", "symbol", "adjusted")]
df <- df %>% spread(symbol, adjusted)
ind_shift <- which(colnames(df) %in% currency_symbs)
df[, ind_shift] <- rbind(rep(NA, length(ind_shift)), df[-nrow(df), ind_shift])
df <- df[-c(1, nrow(df)), ]
date_vec <- df$date
mat_ts_dy <- na.approx(df[, -1])
row.names(mat_ts_dy) <- as.character(date_vec)
#o <- apply(mat_ts_dy, 2, function(x) length(which(is.na(x))))
#table(o)
#which(o==1)
#xts_ts_dy <- xts(mat_ts_dy, date_vec)
#date_vec <- index(xts_ts_dy)
#-----
mat_pctDiff_dy <- diff(mat_ts_dy) / mat_ts_dy[-nrow(mat_ts_dy), ] * 100
#mat_pctDiff_dy <- diff(log(mat_ts_dy))
row.names(mat_pctDiff_dy) <- as.character(date_vec[-1])
ts_avg_dy <- rowMeans(mat_pctDiff_dy)
mu_ret_dy <- colMeans(mat_pctDiff_dy)
sd_ret_dy <- apply(mat_pctDiff_dy, 2, sd)

# mat_ts_wk <- to.weekly(xts(mat_ts_dy, date_vec), OHLC = F)
# mat_pctDiff_wk <- diff(mat_ts_wk) / mat_ts_wk[-nrow(mat_ts_wk), ] * 100
# mat_pctDiff_wk <- as.matrix(mat_pctDiff_wk[-1, ])
# ts_avg_wk <- rowMeans(mat_pctDiff_wk)
# mu_ret_wk <- colMeans(mat_pctDiff_wk)
# sd_ret_wk <- apply(mat_pctDiff_wk, 2, sd)
# 
# mat_ts_mo <- to.monthly(xts(mat_ts_dy, date_vec), OHLC = F)
# mat_pctDiff_mo <- diff(mat_ts_mo) / mat_ts_mo[-nrow(mat_ts_mo), ] * 100
# mat_pctDiff_mo <- as.matrix(mat_pctDiff_mo[-1, ])
# ts_avg_mo <- rowMeans(mat_pctDiff_mo)
# mu_ret_mo <- colMeans(mat_pctDiff_mo)
# sd_ret_mo <- apply(mat_pctDiff_mo, 2, sd)
#----------------------------------------------
# mu_pctRet <- apply(10^-2 * mat_pctDiff_dy, 2, function(x) prod(1 + x)) - 1
# mu_pctRet_check <- (mat_ts_dy[nrow(mat_ts_dy), ] - mat_ts_dy[1, ]) / mat_ts_dy[1, ]
# mu_pctRet / mu_pctRet_check
#----------------------------------------------
# Compare histograms of daily, weekly, and monthly returns
# df_plot <- data.frame(ts_id = names(mu_ret_dy), mu_ret_dy, mu_ret_wk, mu_ret_mo)
# colnames(df_plot)[-1] <- c("Mean Daily Return", "Mean Weekly Return", "Mean Monthly Return")
# gathercols <- colnames(df_plot)[-1]
# df_plot <- df_plot %>% gather_("Return Type", "Value", gathercols)
# #gg <- ggplot(df_plot, aes(Value, fill = `Return Type`))
# gg <- ggplot(df_plot, aes(Value))
# gg <- gg + geom_density(alpha = 0.4)
# gg <- gg + facet_wrap(~`Return Type`, nrow = 1, scales = "free")
# #gg <- gg + coord_cartesian(xlim = c(-0.02, 0.02))
# gg
#----------------------------------------------
mat_ts <- mat_ts_dy
colnames(mat_ts) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
mat_pctDiff <- mat_pctDiff_dy
#mat_pctDiff <- mat_pctDiff_wk
date_vec <- row.names(mat_pctDiff)
#length(date_vec)
row.names(mat_pctDiff) <- as.character(date_vec)
colnames(mat_pctDiff) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
check_on_data <- F
if(check_on_data){
  df_plot <- as.data.frame(mat_pctDiff)
  gathercols <- colnames(df_plot)
  df_plot$Date <- date_vec
  df_plot <- df_plot %>% gather_("Security", "Weekly change (%)", gathercols)
  df_plot$Type <- NA
  df_plot$Type[which(df_plot$Security %in% spy_sector_detail)] <- "US Sectors"
  df_plot$Type[which(df_plot$Security %in% minerals_detail)] <- "Minerals"
  df_plot$Type[which(df_plot$Security %in% agriculture_detail)] <- "Agriculture"
  df_plot$Type[which(df_plot$Security %in% energy_detail)] <- "Energy"
  df_plot$Type[which(df_plot$Security %in% currency_detail)] <- "Major Currency Pairs"
  df_plot$Type[which(df_plot$Security %in% emerg_mkt_detail)] <- "Emerging Markets"
  df_plot$Type[which(df_plot$Security %in% crypto_detail)] <- "Cryptocurrencies/\nBlockchain"
  df_plot$Type[which(df_plot$Security %in% Tbond_detail)] <- "T-bonds"
  #------------------------------------------------------------
  # gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Security, color = Security))
  # gg <- gg + geom_line()
  # gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
  # gg <- gg + theme(legend.position = "none")
  # gg
  #------------------------------------------------------------
  this_type <- "US Sectors"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Minerals"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Agriculture"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Energy"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Major Currency Pairs"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Emerging Markets"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Cryptocurrencies/Blockchain"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "T-bonds"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
}

#=============================================================================
#-----------------------------------------------------------------------
# For a good illustrative example of portfolio optimization, select a financial period over which the returns in the portfolio are as varied as possible yet with the minimum return being >= 2%. This can be achieved by adjusting "buffer". Also'd be nice to have all signal period returns > 0, but this does not seem possible in this data set.
#-----------------------------------------------------------------------
# ind_train <- 1:round(nrow(mat_pctDiff) * 1 / 2)
# ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
length_Q <- 67 # Length of a financial quarter (3 months)
buffer <- 5#36 # Select to give high cv of returns but with min return >=1-2%
#------------------------------------------------------------
# min_pctRet <- c()
# cv_pctRet <- c()
# buffr <- c()
# n_negRet_sigs_train <- c()
# n_negRet_sigs_test <- c()
# for(i in 1:110){
# buffer <- 2+i
#------------------------------------------------------------
mat_ts_eg <- mat_ts
ind_train <- (nrow(mat_ts_eg) - 2 * length_Q - buffer + 1):(nrow(mat_ts_eg) -  length_Q - buffer)
ind_test <- (nrow(mat_ts_eg) - length_Q - buffer + 1):(nrow(mat_ts_eg) - buffer)
mat_ts_eg_train <- mat_ts_eg[ind_train, ]
mat_ts_eg_test <- mat_ts_eg[ind_test, ]
#------------------------------------------------------------
# Get returns barchart df organized by group
nab_decRet_eg_train <- (mat_ts_eg_train[nrow(mat_ts_eg_train), ] - mat_ts_eg_train[1, ]) / mat_ts_eg_train[1, ]
nab_decRet_eg_test <- (mat_ts_eg_test[nrow(mat_ts_eg_test), ] - mat_ts_eg_test[1, ]) / mat_ts_eg_test[1, ]
nab_pctRet_eg_train <- 100 * nab_decRet_eg_train
nab_pctRet_eg_test <- 100 * nab_decRet_eg_test
#---
# check
# mat_pctDiff <- 100 * diff(mat_ts_eg_train) / mat_ts_eg_train[-nrow(mat_ts_eg_train), ]
# nab_pctRet_check <- 100 * (apply(10^-2 * mat_pctDiff, 2, function(x) prod(1 + x)) - 1)
# nab_pctRet / nab_pctRet_check
#------------------------------------------------------------
mat_decDiff_eg_train <- diff(mat_ts_eg_train) / mat_ts_eg_train[-nrow(mat_ts_eg_train), ]
mat_decDiff_eg_test <- diff(mat_ts_eg_test) / mat_ts_eg_test[-nrow(mat_ts_eg_test), ]
mat_pctDiff_eg_train <- 100 * mat_decDiff_eg_train
mat_pctDiff_eg_test <- 100 * mat_decDiff_eg_test
#------------------------------------------------------------
train_start_date <- row.names(mat_pctDiff_eg_train)[1]
train_stop_date <- row.names(mat_pctDiff_eg_train)[nrow(mat_pctDiff_eg_train)]
test_start_date <- row.names(mat_pctDiff_eg_test)[1]
test_stop_date <- row.names(mat_pctDiff_eg_test)[nrow(mat_pctDiff_eg_test)]

```


In principal components analysis, a dataset $X$ containing $t$ observations of $n$ variables is distilled into a dataset $S$ of just $m<n$ variables that capture the main tendencies and structure in the data.^[The data is always centered. If the variables in $X$ follow diverse scalings and/or units of measurement (i.e. if apples are being compared to oranges), then $X$ should also be scaled to unit variance. In this exposition, the variables are all of the same type, and so $X$ is centered but not scaled. See Abdi [-@abdi2010principal] for an introduction to principal components analysis.] The distilled matrix $S$ is defined

\begin{equation}
S = X \tilde{P}
\label{eq:sigs_def}
\end{equation}

Where $\tilde{P}$ is a matrix containing the $m$ leading eigenvectors of the full set of eigenvectors $P$, which is taken from the eigendecomposition of the data covariance matrix $\Sigma_{XX}$ (equation \ref{eq:eigDecomp}).

\begin{equation}
\Sigma_{XX} = P\Gamma P'
\label{eq:eigDecomp}
\end{equation}

Where $\Gamma$ is the diagonal matrix of the eigenvalues of $\Sigma_{XX}$. 

From the definition (\ref{eq:sigs_def}), it follows that the columns of $S$ are uncorrelated with each other, and that their variance is given by the leading $m$ eigenvalues of the data covariance matrix.

\begin{equation}
\begin{split}
\Sigma_{SS} &= \frac{1}{n-1}S'S \\
&= \frac{1}{n-1} \tilde{P}'X'X\tilde{P} \\
&= \tilde{P}'\Sigma_{XX}\tilde{P} \\
&= \tilde{P}'P \Gamma P'\tilde{P}=\tilde{\Gamma}
\end{split}
\label{eq:covmat_SS}
\end{equation}

The columns of the distilled matrix $S$ are often referred to as the principal components (PC), or the PC scores, or the factor scores. When dealing with noisy time series, as in this paper, they might just as well be referred to as the "signals", in the sense that they are signals extracted from noise.

There then remains the question of what essential process these dimensions or signals $S$ describe. This can be interpreted based on how correlated they are with the variables in $X$. These correlations ($K_{XS}$) are found by first finding their corresponding covariances ($\Sigma_{XS}$).

\begin{equation}
\begin{split}
\Sigma_{XS} &= \frac{1}{n-1}X'S \\
&= \frac{1}{n-1}X'X\tilde{P} \\
&= K\tilde{P} = P \Gamma P'\tilde{P} \\
&= \tilde{P} \tilde{\Gamma}
\end{split}
\end{equation}

The correlation matrix $K_{XS}$ then follows as

\begin{equation}
\begin{split}
K_{XS} &= D(\boldsymbol{\sigma}_X)^{-1} \Sigma_{XS} D(\boldsymbol{\sigma}_S)^{-1} \\
&= D(\boldsymbol{\sigma}_X)^{-1} \tilde{P} \tilde{\Gamma} D(\boldsymbol{\sigma}_S)^{-1}
\end{split}
\end{equation}

(Where the notation $D(\boldsymbol{\sigma}_X)$ stands for a diagonal matrix with the vector $\boldsymbol{\sigma}_X$ along the diagonal.) But the standard deviations of the signals are just the square roots of the retained eigenvalues (recall equation \ref{eq:covmat_SS}), so this reduces to

\begin{equation}
\begin{split}
K_{XS} &= D(\boldsymbol{\sigma}_X)^{-1}\Sigma_{XS} \tilde{\Gamma}^{-{1 \over 2}} \\
&= D(\boldsymbol{\sigma}_X)^{-1}\tilde{P} \tilde{\Gamma} \tilde{\Gamma}^{-{1 \over 2}} \\
&= D(\boldsymbol{\sigma}_X)^{-1}\tilde{P} \tilde{\Gamma} ^{1 \over 2}
\end{split}
\end{equation}

Note that if $X$ were scaled to unit variance, then this would reduce further to

\begin{equation}
K_{XS} = \tilde{P} \tilde{\Gamma}^{1 \over 2}
\end{equation}

The correlations matrix $K_{XS}$ is sometimes referred to as the "loadings" matrix, in the sense that it indicates how much each variable in $X$ loads onto a given signal (or, vice versa, how much each signal loads onto a given variable).^[Although many prefer to call $P$ the loadings.] In keeping with this convention, and in order to reduce notational clutter, $K_{XS}$ is henceforth relabeled $L$.

The correlations between the financial assets in the example from Figure \@ref(fig:mvConv) above and their four leading signals are presented in Figure \@ref(fig:corrXSBarchart). Concrete meaning can be attributed to each of these otherwise abstract signals by examining how correlated they are with the portfolio items.


```{r corrXSBarchart, fig.show='hold', fig.width=5, fig.height=3, fig.align='center', fig.cap="Correlation of portfolio items with leading signals."}


# Define function to calculate variable-signal correlations, financial data
get_S_and_corrXS <- function(mat_X_in){
  # mat_P = eigenvectors of the data correlation matrix
  # mat_G = corresponding eigenvalues
  mat_X_centered <- scale(mat_X_in, scale = F)
  # out_svd <- svd(mat_X_centered)
  # sing_values <- out_svd$d
  # n_obs <- nrow(mat_X_centered)
  # eig_values <- sing_values^2 / (n_obs - 1)
  # mat_P <- out_svd$v
  
  mat_P <- eigen(cov(mat_X_in))$vectors
  if(mean(mat_P[, 1]) < 0){mat_P <- -mat_P}
  eig_values <- eigen(cov(mat_X_in))$values
  mat_G <- diag(eig_values)
  
  #mat_P_sigs <- mat_P[, 1:n_signals]
  # eig_values[1:n_signals] / eigen(cov(mat_X_centered))$values[1:n_signals] #check
  # mat_P / eigen(cov(mat_X_in))$vectors #check
  
  #mat_G <- diag(eig_values)
  
  #mat_G_sigs <- matU[, 1:n_signals]
  #---------------------------------------------
  sd_X <- apply(mat_X_in, 2, sd)
  D_sdX_inv <- diag(1 / sd_X)
  cormat_XS <- D_sdX_inv %*% mat_P %*% sqrt(mat_G)
  row.names(cormat_XS) <- colnames(mat_X_in)
  mat_L <- cormat_XS
  #mat_L <- diag(1 / apply(mat_X_in, 2, sd)) %*% mat_P %*% sqrt(mat_G)
  #---------------------------------------------------------
  # Set sign of eigenvectors such that signals best conform to their most highly correlated items
  # First have to get average of highest correlated items for each signal
  corrThresh <- 0.55
  n_items <- ncol(mat_L)
  list_X_hiCorr_avg <- list()
  for(i in 1:n_items){
    this_loadvec <- mat_L[, i]
    ind_tracks <- which(abs(this_loadvec) >= corrThresh)
    if(length(ind_tracks) == 0){
      ind_tracks <- which(abs(this_loadvec) == max(abs(this_loadvec)))
    }
    if(length(ind_tracks) == 1){
      list_X_hiCorr_avg[[i]] <- mat_X_centered[, ind_tracks]
    }else{
      loadvec_kept <- this_loadvec[ind_tracks]
      list_X_hiCorr_avg[[i]] <- rowMeans(mat_X_centered[, ind_tracks])
      
    }
  }
  mat_X_hiCorr_avg <- do.call(cbind, list_X_hiCorr_avg)
  mat_S_all <- mat_X_centered %*% mat_P
  #mat_S_all <- mat_X_in %*% mat_P
  for(i in 1:n_items){
    this_S <- mat_S_all[, i]
    this_X_hiCorr_avg <- mat_X_hiCorr_avg[, i]
    mse <- mean((this_S - this_X_hiCorr_avg)^2)
    mse_neg <- mean((-this_S - this_X_hiCorr_avg)^2)
    if(mse_neg < mse){
      mat_P[, i] <- -mat_P[, i]
    }
  }
  cormat_XS <- D_sdX_inv %*% mat_P %*% sqrt(mat_G)
  row.names(cormat_XS) <- colnames(mat_X_in)
  mat_L <- cormat_XS
  mat_S_all <- mat_X_centered %*% mat_P
  #---------------------------------------------
  # res <- FactoMineR::PCA(mat_pctDiff_in, scale.unit = F, ncp = ncol(mat_pctDiff_in), graph = F)
  # mat_L_FactoMiner <- res$var$coord
  # mat_L / mat_L_FactoMiner
  
  list_out <- list(mat_S_all, cormat_XS, eig_values, mat_P)
  return(list_out)
}
#====================================================
# Barchart of variable-signal correlations
plot_corrXS_barchart <- function(mat_L, group_info = NULL, xAxis_title = NULL, sigNames = NULL){
  
  n_signals <- ncol(mat_L)
  df_plot <- data.frame(Item = row.names(mat_L), mat_L)
  df_plot$Item <- as.character(df_plot$Item)
  #-------------------------------------------------------
  if(is.null(sigNames)){
    signal_id <- paste("Signal", 1:n_signals)
  }else{
    signal_id <- paste("Signal", 1:n_signals, "\n", sigNames)
  }
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  #-------------------------------------------------------
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Correlation", gathercols)
  df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))
  
  if(!is.null(group_info)){
    outlist <- group_fn(group_info)
    cols_ordered_by_group <- outlist[[1]]
    group_color_vec <- outlist[[2]]
    group_vec_ordered <- outlist[[3]]
    df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
    df_plot <- merge(df_plot, df_match_group, by = "Item")
    df_plot <- df_plot[order(df_plot$Group), ]
    df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item))
    gg <- ggplot(df_plot, aes(x = Item, y = Correlation, fill = Group))
    gg <- gg + scale_fill_manual(values = unique(group_color_vec))
  }else{
    gg <- ggplot(df_plot, aes(x = Item, y = Correlation))
  }
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + ylim(limits = c(-1, 1))
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  if(!is.null(xAxis_title)){
    gg <- gg + labs(y = xAxis_title)
  }
  gg <- gg + theme(axis.text = element_text(size = 7),
                   axis.title.x = element_text(size = 7),
                   axis.title.y = element_blank(),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 7),
                   strip.text = element_text(size = 7))
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  gg
  
}

#====================================================
n_signals <- 4
mat_X_in <- mat_pctDiff_eg_train
#mat_X_in <- mat_ts_eg_train
list_out <- get_S_and_corrXS(mat_X_in)
mat_S_all <- list_out[[1]]
cormat_XS <- list_out[[2]]
eig_values <- list_out[[3]]
mat_P <- list_out[[4]]
mat_S <- mat_S_all[, 1:n_signals]
mat_L <- cormat_XS[, 1:n_signals]

#eigen(cov(mat_S))$values / eig_values[1:n_signals]

plot_corrXS_barchart(mat_L)

```

Signal 4, for example, appears to have something to do with price movements in the communications sector. Signal 3, meanwhile, is concerned with Utilities and Real Estate, and so might be called the "Housing & Urban Development Signal". The interpretation of Signals 1 and 2, on the other hand, is not so straightforward, since they are correlated with many portfolio items. In such cases, it is often useful to apply an orthogonal rotation to $L$ in order to clarify the picture. That is to say, instead of examining $L$, one examines $L_{\circlearrowleft}$.

\begin{equation}
L_{\circlearrowleft} = LB
\label{eq:Lrot}
\end{equation}

Where $B$ is the orthogonal rotation matrix, such that $B'B = I$ and $BB' = I$.
<!-- Note that a rotation of $L$ implies a rotation of the signals, since -->

<!-- \begin{equation} -->
<!-- SR = XP -->
<!-- \end{equation} -->

In Figure \@ref(fig:corrXSBarchartRot) a special kind of orthogonal rotation, called a varimax rotation, is applied to $L$. Varimax rotations flesh out structure by maximizing sparseness in the rotated matrix. After applying this rotation, Signal 1 is more clearly representative of Biotechnology and Healthcare, and so might be called the "Pharmaceutical Signal". Signal 2 loadings are also now more distinctly pronounced, especially Financials, Industrial, and Transportation. Signal 2 might thus be called the "Financial and Physical Infrastructure Signal."^[Gopikrishnan, Rosenow, Plerou, and Stanley [-@gopikrishnan2001quantifying] pursued a similar line of inquiry when they looked at the components of the eigenvectors of a financial data correlation matrix. However, they did not explain that their findings are indicative of PC-asset correlations; nor did they apply a varimax rotation to clarify the interpretation.]

Further visual confirmation of these interpretations of signal meaning is given by plotting the signals in the time domain together with their highest loading portfolio items superimposed (Figure \@ref(fig:signalsWithHiCorrItems)). Note how the highest loading items tend to hew closely to their respective signals.

```{r corrXSBarchartRot, fig.show='hold', fig.width=5, fig.height=3, fig.align='center', fig.cap="Varimax rotated correlation of portfolio items with leading signals.", echo=F}

mat_Lrot <- varimax(mat_L)[[1]]
mat_Lrot <- matrix(as.numeric(mat_Lrot),
                   attributes(mat_Lrot)$dim,
                   dimnames = attributes(mat_Lrot)$dimnames)
mat_R <- varimax(mat_L)[[2]]
mat_R <- matrix(as.numeric(mat_R),
                attributes(mat_R)$dim,
                dimnames = attributes(mat_R)$dimnames)

xAxis_title <- "Varimax Rotated Correlation"
plot_corrXS_barchart(mat_Lrot, group_info = NULL, xAxis_title, sigNames = NULL)


```

```{r signalsWithHiCorrItems, fig.show='hold', fig.width=6, fig.height=5, fig.align='left', fig.cap="Signals (thick grey lines) plotted together with their most highly correlated assets."}
# Define function to plot signals with their highest loading items.
plot_sigs_wHiLoad_items <- function(mat_X_in, mat_S, mat_L, color_vec, sigNames = NULL, load_threshold = 0.55, n_display_max = 4){
  n_signals <- ncol(mat_L)
  if(is.null(sigNames)){
    fig_title_vec <- paste("Signal", 1:n_signals)
  }else{
    fig_title_vec <- paste("Signal", 1:n_signals, "-", sigNames)
  }
  # Plot signals against highest correlated variables
  date_vec <- row.names(mat_S)
  xAxis_labels <- date_vec[seq(1, nrow(mat_S), length.out = 5)]
  list_gg <- list()
  for(i in 1:n_signals){
    this_loadvec <- mat_L[, i]
    ind_tracks <- which(abs(this_loadvec) >= load_threshold)
    if(length(ind_tracks) == 0){
      ind_tracks <- which(abs(this_loadvec) == max(abs(this_loadvec)))
    }
    
    n_display <- length(ind_tracks)
    loadvec_kept <- this_loadvec[ind_tracks]
    if(n_display > n_display_max){
      n_to_omit <- n_display - n_display_max
      #random_omission <- sample(1:n_display, n_to_omit)
      #mat_pctDiff_tracks <- mat_pctDiff_tracks[, -random_omission]
      ind_omit <- order(loadvec_kept)[c(1:n_to_omit)]
      ind_tracks <- ind_tracks[-ind_omit]
      loadvec_kept <- this_loadvec[ind_tracks]
    }
    mat_X_tracks <- mat_X_in[, ind_tracks]
    color_vec_tracks <- color_vec[ind_tracks]
    #------------
    df_plot_sig <- data.frame(Date = date_vec, Value = mat_S[, i])
    df_plot_tracks <- data.frame(Date = date_vec, mat_X_tracks)
    if(ncol(df_plot_tracks) > 2){
      gathercols <- colnames(mat_X_tracks)
      colnames(df_plot_tracks)[-1] <- gathercols
      df_plot_tracks <- df_plot_tracks %>% gather_("ts", "Value", gathercols)
    }else{
      colnames(df_plot_tracks)[-1] <- "Value"
      df_plot_tracks$ts <- colnames(mat_X_in)[ind_tracks]
    }
    #------------
    gg <- ggplot()
    gg <- gg + geom_line(data = df_plot_sig, aes(x = Date, y = Value, group = 1), color = "grey", lwd = 1.3)
    gg <- gg + geom_line(data = df_plot_tracks, aes(x = Date, y = Value, group = ts, color = ts))
    gg <- gg + scale_color_manual(values = color_vec_tracks)
    gg <- gg + scale_x_discrete(breaks = xAxis_labels)
    gg <- gg + labs(title = fig_title_vec[i])
    
    if(i == n_signals){
      #gg <- gg + labs(title = paste("Signal", i), caption = fig_title)
      gg <- gg + theme(axis.text.x = element_text(size = 7),
                       axis.text.y = element_text(size = 7),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.title = element_blank(),
        legend.text = element_text(size = 7),
        plot.title = element_text(size = 7)
        #plot.caption = element_text(hjust = 0, size = 10)
      )
    }else{
      gg <- gg + theme(axis.text.x = element_blank(),
                       axis.text.y = element_text(size = 7),
                       axis.title.x = element_blank(),
                       axis.title.y = element_blank(),
                       legend.title = element_blank(),
                       legend.text = element_text(size = 7),
                       plot.title = element_text(size = 7))
    }
    
    list_gg[[i]] <- gg
    
  }
  
  gg_all <- wrap_plots(list_gg) + plot_layout(ncol = 1)
  gg_all
}

# Name signals based on correlations
# sigNames <- c("Portfolio average\n(Especially Pharmaceutical, Technology, and Luxury Goods)",
#               "Transportation and Industrial",
#               "Housing & Urban Development (Inverse)",
#               "Communications (Inverse)")
# 
# mat_X_in <- mat_pctDiff_in
# plot_sigs_wHiLoad_items(mat_X_in, mat_S, mat_L, sigNames, load_threshold = 0.55, n_display_max = 4)
sigNames <- c("Pharmaceutical",
              "Financial and Physical Infrastructure",
              "Housing & Urban Development",
              "Communications")

n_items <- length(ts_symb_vec)
bag_of_colors <- randomcoloR::distinctColorPalette(k = 2 * n_items)
color_vec <- sample(bag_of_colors, n_items)
plot_sigs_wHiLoad_items(mat_X_in, mat_S, mat_Lrot, color_vec, sigNames, load_threshold = 0.65, n_display_max = 4)

```  

A calculation of varimax loadings over data from the subsequent period (`r test_start_date` to `r test_stop_date`), presented in Figure \@ref(fig:corrXSBarchartRotTest), suggests that signal composition remains fairly stable from one period to the next, although signal order may change. In this case, the Financial and Physical Infrastructure Signal has changed places with the Pharmaceutical Signal. The order of signals reflects their relative predominance in shaping the overall variance in the dataset.

```{r corrXSBarchartRotTest, fig.show='hold', fig.width=5, fig.height=3, fig.align='center', fig.cap="Varimax rotated correlation of portfolio items with leading signals, financial data from subsequent period.", echo=F}
mat_X_in_test <- mat_pctDiff_eg_test
list_out <- get_S_and_corrXS(mat_X_in_test)
mat_S_test_all <- list_out[[1]]
cormat_XS_test <- list_out[[2]]
eig_values_test <- list_out[[3]]
mat_P_test <- list_out[[4]]
#----

mat_L_test <- cormat_XS_test[, 1:n_signals]
mat_Lrot_test <- varimax(mat_L_test)[[1]]
mat_Lrot_test <- matrix(as.numeric(mat_Lrot_test),
                        attributes(mat_Lrot_test)$dim,
                        dimnames = attributes(mat_Lrot_test)$dimnames)
mat_R_test <- varimax(mat_L_test)[[2]]
mat_R_test <- matrix(as.numeric(mat_R_test),
                     attributes(mat_R_test)$dim,
                     dimnames = attributes(mat_R_test)$dimnames)

xAxis_title <- "Varimax Rotated Correlation (test data)"
plot_corrXS_barchart(mat_Lrot_test[, 1:4], group_info = NULL, xAxis_title, sigNames = NULL)

```

In practice, the number of signals that should be distilled from the original data set $X$ depends upon how much of the variance in $X$ the researcher wishes to capture or reflect in the signals, and how many signals are required to reach this subjectively determined threshold. The portion of the system's evolution reflected in any given signal ($c_i$) is defined as the signal's variance divided by the sum of all signal variances. Recalling from equation \@ref(eq:covmat_SS) that a signal's variance is just the corresponding eigenvalue $\gamma_i$ extracted from $\Sigma_{XX}$, this is expressed

\begin{equation}
c_i = \frac{u_i}{\sum_{i = 1}^n \gamma_i}
\end{equation}

The cumulative variance captured by a group of $k$ signals is then

\begin{equation}
c_k <- \frac{\sum_{i=1}^k \gamma_i}{\sum_{i = 1}^n \gamma_i}
\end{equation}

The individual and cumulative portions explained by each signal are plotted in Figure \@ref(fig:varExplainedBarchart). Customarily, researchers like to retain signals such that at least 90% of the variance in the original data is explained. The horizontal dashed line in the plot marks this subjective threshold.

```{r varExplainedBarchart, fig.show='hold', fig.width=5, fig.height=2, fig.align='center', fig.cap="Plot of the individual and cumulative portions of variance explained by each signal.", echo=F}
c_vec <- eig_values / sum(eig_values)
ck_vec <- cumsum(c_vec)
df_plot <- data.frame(Signal = paste("Signal", 1:length(eig_values)), Portion_explained = c_vec, Portion_explained_cumul = ck_vec)
colnames(df_plot)[2:3] <- c("Individually", "Cumulatively")
gathercols <- colnames(df_plot)[2:3]
df_plot <- gather_(df_plot, "Portion explained", "Value", gathercols)
df_plot$Signal <- factor(df_plot$Signal,
                         levels = unique(df_plot$Signal),
                         ordered = T)
gg <- ggplot(df_plot, aes(x = Signal, y = Value, fill = `Portion explained`))
gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
gg <- gg + scale_fill_manual(values = c("wheat3", "paleturquoise"))
gg <- gg + geom_hline(yintercept = 0.9, color = "coral", size = 1, linetype = "dashed")
gg <- gg + theme(axis.text.y = element_text(size = 7),
                 axis.text.x = element_text(size = 7, angle = 60, hjust = 1),
                 axis.title = element_blank(),
                 legend.title = element_text(size = 8),
                 legend.text = element_text(size = 8))
gg
#====================================================
n_signals <- which(ck_vec > 0.9)[1]
mat_S <- mat_S_all[, 1:n_signals]
mat_L <- cormat_XS[, 1:n_signals]
eig_values_sigs <- eig_values[1:n_signals]
#====================================================

```

The plot shows that the leading `r n_signals` signals are sufficient to meet this criterion.

## Approximating the data correlation matrix from the loadings matrix

Note that an approximate data correlation matrix ($\tilde{K}_{XX}$) can be computed from these retained signals, as the outer product of $L_{\circlearrowleft}$ with itself.

\begin{equation}
\begin{split}
L_{\circlearrowleft}L_{\circlearrowleft}' &= LB (LB)' = LBB'L' = LL' \\
&= (D(\sigma_X)^{-1} \tilde{P} \tilde{\Gamma}^{-{1 \over 2}}) (D(\sigma_X)^-1 \tilde{P} \tilde{\Gamma}^{-{1 \over 2}})' \\
&= (D(\sigma_X)^{-1} \tilde{P} \tilde{\Gamma}^{-{1 \over 2}}) \tilde{\Gamma}^{-{1 \over 2}} \tilde{P}' D(\sigma_X)^{-1} \\
&= D(\sigma_X)^{-1} \tilde{\Sigma}_{XX} D(\sigma_X)^{-1} \\
&= \tilde{K}_{XX}
\end{split}
\label{eq:cormat_from_Lrot}
\end{equation}

(The approximate covariance matrix ($\tilde{\Sigma}_{XX}$) is then of course obtained by pre and post multiplying $\tilde{K}_{XX}$ by $D(\boldsymbol{\sigma}_X)$.)

The difference between the financial data correlation matrix and the correlation matrix calculated in equation \@ref(eq:cormat_from_Lrot) is shown in Figure \@ref(fig:compareCorMats). Note that the difference is remarkably small for most entries. The signals derived correlation matrix is approximate in the sense that it approximates the data correlation matrix; but it should not necessarily be considered inferior in terms of accuracy. To the extent that the original data are contaminated by noise, the signals derived correlation matrix may prove more accurate with respect to the "true process" that generates the data.

```{r compareCorMats, fig.show='hold', fig.width=5, fig.height=3, fig.align='center', fig.cap="The data correlation matrix minus the correlation matrix derived from the retained signals, financial data."}
# Define function to plot covariance/correlation matrices
plot_covmat <- function(covmat, fig_title = "Covariance Matrix", round_to = 2, graph_on = T){
  covmat[upper.tri(covmat)] <- NA
  df_plot <- covmat %>% tbl_df()
  these_levels <- colnames(df_plot)
  df_plot$ItemX <- colnames(df_plot)
  gathercols <- colnames(df_plot)[-ncol(df_plot)]
  df_plot <- df_plot %>% gather_("ItemY", "Value", gathercols)
  df_plot$ItemX <- factor(df_plot$ItemX, levels = these_levels)
  df_plot$ItemY <- factor(df_plot$ItemY, levels = these_levels)
  
  #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
  midpoint <- 0
  gg <- ggplot(df_plot, aes((ItemX), (ItemY)))
  gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
  gg <- gg + geom_text(aes(label = round(Value, round_to)), size = 2.5)
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
  }
  gg <- gg + theme_bw()
  gg <- gg + theme(axis.text.x = element_text(size = 7, angle = 60, hjust = 1),
                   axis.text.y = element_text(size = 7),
                   axis.title = element_blank(),
                   legend.title = element_blank(),
                   plot.title = element_text(face = "bold", size = 8))
  gg <- gg + scale_fill_gradient2(low = "khaki", mid = "cyan", high = "magenta", midpoint, na.value = "white")
  if(graph_on){print(gg)}
  return(gg)
  
}
#=======================================================================

cormat_XX <- cor(mat_pctDiff_eg_train)
cormat_XX_derived <- mat_L %*% t(mat_L)
mat_diff_cormats <- cormat_XX - cormat_XX_derived

plot_covmat(mat_diff_cormats, fig_title = NULL, graph_on = F)

```

Conversely, the signals covariance matrix ($\tilde{\Gamma}$) can be obtained from the eigendecomposition of the inner product $D(\boldsymbol{\sigma}_X) L_{\circlearrowleft}'L_{\circlearrowleft}D(\boldsymbol{\sigma}_X)$.

\begin{equation}
\begin{split}
\label{eq:crowdsource_G}
D(\boldsymbol{\sigma}_X) L_{\circlearrowleft}'L_{\circlearrowleft}D(\boldsymbol{\sigma}_X) &= (\tilde{P} \tilde{\Gamma}^{1 \over 2} B)' \tilde{P} \tilde{\Gamma}^{1 \over 2} B \\
&= B' \tilde{\Gamma}^{1 \over 2} \tilde{P}' \tilde{P} \tilde{\Gamma}^{1 \over 2} B \\
&= B' \tilde{\Gamma} B
\end{split}
\end{equation}

Note that the implicit orthogonal rotation $B$ is also recovered as the transpose of the corresponding eigenvectors. Moreover, with $\tilde{\Gamma}$ and $B$ in hand, it is then possible to derive the implicit leading eigenvectors of the data covariance matrix ($\tilde{P}$). Letting $Q_{\circlearrowleft} = D(\boldsymbol{\sigma}_X) L_{\circlearrowleft}$,

\begin{equation}
\tilde{P} = Q_{\circlearrowleft}B'\tilde{\Gamma}^{-1 / 2}
\label{eq:deducePcrowd}
\end{equation}

## "Reverse engineering" covariance matrices from domain knowledge
<!-- Equations \ref{eq:covmat_SS} to \ref{eq:Lrot} -->
The foregoing discussion implies that it is possible to work backwards from the orthogonally rotated loadings $L_{\circlearrowleft}$ and asset risk $\boldsymbol{\sigma}_X$, to arrive at an approximate data covariance matrix and signals covariance matrix.

This could be useful in situations where the original data $X$ is relatively scarce or suspect, but where domain knowledge is relatively abundant. That is, in the absence of the data ($X$), experts with a great deal of knowledge about the underlying system that would generate the data could be asked to name and rank in order of importance the cross cutting tendencies that describe 90% of (or simply "best describe") the general evolution of the system. These cross-cutting tendencies could then be interpreted as signals. The experts could then be asked how influential each portfolio item is---on a scale, say, of -100 to 100---in shaping (or being shaped by) the direction of these tendencies. The experts' responses could then be divided by 100 and interpreted as correlations between signals and portfolio items. The result of this expert consultation is could thus be considered the rotated loadings matrix ($L_{\circlearrowleft}$) of an unobserved dataset.^[The expert survey response must be considered an _orthogonally rotated_ loadings matrix, as opposed to a loadings matrix, because loadings matrices are always orthogonal, whereas the elicited loadings will generally not be orthogonal.] Given the risk associated with each portfolio item ($\boldsymbol{\sigma}_X$), which could also be elicited, it is then possible to generate approximate covariance matrices for the unobserved data and corresponding signals via equations \ref{eq:cormat_from_Lrot} and \ref{eq:crowdsource_G}, respectively.

Again, such an approach makes sense only in contexts where a relative lack of data is compensated by a relative abundance of domain knowledge. As a rule of thumb, the appropriateness of the crowdsourcing approach may be assessed by meditating upon the conceptual ratio $\nu$.
<!-- In the AR4D context, the process is the same, except that expected asset return is replaced by the research proposal expected proportionality constants $\boldsymbol{\alpha}$. (Recall that these may be thought of as the inverse of the proposal scalability.) -->
$$
\nu = \frac{\text{confidence in domain knowledge}}{\text{confidence in data}}
$$

As $\nu$ is higher, the crowdsourcing approach to covariance matrix estimation is more suitable. As $\nu$ is lower, it becomes more appropriate to estimate the covariance matrix on the basis of data. For values of $\nu$ close to 1, a mixture of the two approaches might be considered.

Financial contexts, characterized by abundant data and relatively scarce or suspect domain knowledge, will typically have a low $\nu$ value. The crowdsourcing approach is thus inappropriate in such settings, except perhaps as a knowledge validating exercise. The AR4D context, on the other hand, generally has a high $\nu$ value; and is thus an appropriate setting for the crowdsourcing approach.

A financial example of signals, and of how signals define the evolution of portfolio items in a problem space, was given above. But what might signals look like in the AR4D context, and how might these be elicited from experts? Here it is important, first of all, to have a clear idea of what the unobserved dataset $X$ represents. In the financial context, $X$ represents daily (or weekly, or monthly, etc., as the case may be) returns. Given the diminishing returns to investment encountered in the AR4D context, on the other hand, the dataset $X$ must be imagined to capture the evolution of the AR4D proposal proportionality parameters $\boldsymbol{\alpha}$ introduced in the derivation of the diminishing returns function. What, then, are the dimensions that best describe the evolution of these parameters within the particular problem space that is of interest? This can be elicited by asking experts and stakeholders questions such as "What are our key objectives? And what are the key performance indicators by which to measure progress towards those objectives?" In the AR4D context, there is often pre-existing consensus on such questions, which can be found laid out in strategy documents. The Sustainable Development Goals [@desa2016transforming] may be considered a set of dimensions defining a very broad AR4D problem space. The key dimensions of any given AR4D portfolio space could be selected as a subset of these.
<!-- Note, in passing, that the original data set $X$ may itself be considered a set of orthogonally rotated signals constructed from some higher dimensional dataset. This unobserved, higher dimensional dataset could likewise be considered a set of orthogonally rotated signals constructed from some still larget dataset, and so on, ad infinitum. Note that the eigenvalues of the system described by these nested datasets remain invariant under each successive step of dimensional reduction, while the eigenvectors change. In this sense, the eigenvalues of the original---or any---data covariance matrix are the "true" covariance matrix, while the eigenvectors may be considered an extraneous rotation of the true covariance matrix, not intrinsic to the construction of the signals, but rather applied extraneously, as an aid in their interpretation, or as a matter of contextualizing the abstract signals with respect to a particular set of concrete circumstances.  -->

# An illustrative example of AR4D risk adjusted portfolio optimization

AR4D risk adjusted portfolio optimization would come after Step 3 in the resource allocation workflow defined by Mills, either complementing or replacing the ranking of research proposals in order of priority that is slated to occur in Step 4. By this point in the resource allocation workflow, a list of research proposals would have been drawn up, and the expected proportionality parameters of each proposal assessed and quantified. Recall from the discussion in section 2 that a proposal's proportionality constant may be thought of as the inverse of its scalability. Risk, meanwhile, is just the uncertainty surrounding the expected proportionality parameter.

A hypothetical list of such AR4D proposals is presented in Figure \@ref(fig:ExpPctRetExamp, together with their expected proportionality parameters and risk.^[The AR4D proposals are loosely grouped into four categories to facilitate interpretation of the graphics, but there is no strict rule followed, and clearly some overlap, in the grouping.]
```{r ExpPctRetExamp, fig.show = "hold", fig.width = 4, fig.height = 4, fig.align = "center", fig.cap = "Hypothetical AR4D proposal expected proportionality parameters and risk."}
#---------------------------------------------
prop_CSA <- c("Heat Tolerant\nBeans", "Coffee\nAgroforestry", "Digital\nAgriculture", "Low Emission\nSilvopastoral")
econGrowth_CSA <- c(0.11, 0.38, 0.4, -0.35)
econEquality_CSA <- c(0.7, 0.32, 0.71, 0.27)
envSust_CSA <- c(0.6, 0.42, 0.6, 0.8)
nutrition_CSA <- c(0.65, 0.06, 0.01, 0.04)
#---------------------------------------------
# prop_socialCap <- c("Micro Finance\nand Insurance", "Multi-stakeholder\nPlatforms", "Market Access")
# econGrowth_socialCap <- c(0.2, 0.45, 0.34)
# econEquality_socialCap <- c(0.87, 0.62, 0.79)
# envSust_socialCap <- c(0.1, 0.1, 0)
#---------------------------------------------
prop_smallHolder <- c("Cassava for\nBio-ethanol", "Triple Purpose\nSweet Potato", "Dairy\nCooperative", "Multi-stakeholder\nPlatforms")
econGrowth_smallHolder <- c(0.67, -0.31, 0.15, 0.4)
econEquality_smallHolder <- c(0.6, 0.75, 0.83, 0.81)
envSust_smallHolder <- c(0.32, 0.4, -0.52, 0.1)
nutrition_smallHolder <- c(0, 0.76, 0.67, 0.11)
#---------------------------------------------
prop_highYcommod <- c("Mega Maize", "Hyper Rice", "Ultra Cow")#, "Uber Palm")
econGrowth_highYcommod <- c(0.82, 0.86, 0.76)#, 0.88)
econEquality_highYcommod <- c(-0.34, -0.42, -0.63)#, -0.58)
envSust_highYcommod <- c(-0.6, -0.52, -0.67)#, -0.83)
nutrition_highYcommod <- c(0.4, 0.45, 0.53)
#---------------------------------------------
df_CSA <- data.frame(Proposal = prop_CSA, `Economic Growth` = econGrowth_CSA, `Economic Equality` = econEquality_CSA, `Environmental Sustainability` = envSust_CSA, `Nutritional Security` = nutrition_CSA, Group = "Climate Smart\nAgriculture")

# df_socialCap <- data.frame(Proposal = prop_socialCap, `Economic Growth` = econGrowth_socialCap, `Economic Equality` = econEquality_socialCap, `Environmental Sustainability` = envSust_socialCap, Group = "Social Capital")

df_smallHolder <- data.frame(Proposal = prop_smallHolder, `Economic Growth` = econGrowth_smallHolder, `Economic Equality` = econEquality_smallHolder, `Environmental Sustainability` = envSust_smallHolder, `Nutritional Security` = nutrition_smallHolder, Group = "Smallholder\nResilience")

df_highYcommod <- data.frame(Proposal = prop_highYcommod, `Economic Growth` = econGrowth_highYcommod, `Economic Equality` = econEquality_highYcommod, `Environmental Sustainability` = envSust_highYcommod, `Nutritional Security` = nutrition_highYcommod, Group = "High Value Yield\nEnhancement")

list_df <- list(df_highYcommod, df_smallHolder, df_CSA)#, df_socialCap)

df_Lrot <- as.data.frame(do.call(rbind, list_df))
colnames(df_Lrot)[2:4] <- gsub("\\.", " ", colnames(df_Lrot)[2:(ncol(df_Lrot) - 1)])
group_names <- as.character(unique(df_Lrot$Group))
n_groups <- length(group_names)
bag_of_colors <- randomcoloR::distinctColorPalette(k = 2 * n_groups)
group_colors_arb <- sample(bag_of_colors, n_groups)

# Randomly assign an expected pct. return to each AR4D proposal
n_prop <- nrow(df_Lrot)
n_signals <- ncol(df_Lrot) - 2
# inv_scalability_ar4d <- exp(rnorm(n_prop))
# inv_scalability_ar4d <- round(inv_scalability_ar4d / sum(inv_scalability_ar4d), 3) * 100
#inv_scalability_ar4d <- runif(n_prop)
inv_scalability_ar4d <- 10^-2 * c(15.9, 12.7, 17.6, 8.8, 7.4 , 9.0, 10.7, 10.8, 7.8 , 6.8, 9.4) # per unit investment
#inv_scalability_ar4d <- 1 / scalability_ar4d 
#nab_decRet_ar4d <- 10^-2 * inv_scalability_ar4d
#inv_scalability_ar4d <- round(inv_scalability_ar4d / sum(inv_scalability_ar4d), 3) * 100
#inv_scalability_ar4d <- 10^-2 * c(15.9, 12.7, 17.6, 8.8, 7.4 , 9.0, 10.7, 10.8, 7.8 , 6.8, 9.4)
scalability_ar4d <- 1 / inv_scalability_ar4d
#sdX_vec <- 1 / 2 * c(abs(as.matrix(inv_scalability_ar4d) + 2 * as.matrix(rnorm(n_prop))))
#sdX_vec <- 1.61 * c(abs(as.matrix(inv_scalability_ar4d) * as.matrix(runif(n_prop))))
sdX_vec <- 10^-2 * c(21.2785349, 9.5972656, 18.4114015, 9.4742876, 2.7923296, 0.6912369, 5.9851785, 12.0813232, 7.0012194, 6.2880289, 7.1941543)
#sdX_vec <- 1 / 2 * c(12.868125, 12.490252, 11.344356, 11.007095, 4.783202, 10.734870, 9.144469, 8.444207, 5.876558, 7.155542, 12.647645)
df_pctRet <- data.frame(inv_scalability_ar4d, Item = df_Lrot$Proposal, Group = df_Lrot$Group)
names(inv_scalability_ar4d) <- df_Lrot$Proposal
# Plot expected returns for each AR4D proposal
group_colors <- group_colors_arb
#plot_returns_barchart(df_pctRet, group_colors)
# list_graph_options <- list()
# list_graph_options[["fig_title"]] <- "AR4D proposal proportionality parameters"
# list_graph_options[["ylab"]] <- NULL
# list_graph_options[["legend_position"]] <- "none"
# list_graph_options[["axisTextX_off"]] <- T
# gg_perRet <- plot_returns_barchart(df_pctRet, group_colors, list_graph_options, graph_on = F)
# 
# df_sd <- data.frame(sdX_vec, Item = df_Lrot$Proposal, Group = df_Lrot$Group)
# list_graph_options[["fig_title"]] <- "Risk (standard deviation)"
# list_graph_options[["legend_position"]] <- NULL
# list_graph_options[["axisTextX_off"]] <- NULL

# gg_perSd <- plot_returns_barchart(df_sd, group_colors, list_graph_options, graph_on = F)
# 
# gg_perRet / gg_perSd


```
In addition to the proposal proportionality parameters and their associated uncertainty, the method introduced in this paper requires elicitation of proposal-signal correlations. Elicitation can take place in a variety of ways, for example: online surveys, in-person conferences, teleconferences, consultation of literature, or some combination thereof. It should be explained to participants that a positive proposal-signal correlation means the proposal contributes toward the signal, while a negative rating means the proposal works against the signal; and a correlation of zero means that the proposal has no influence upon the given signal one way or the other. Again, the language used in this survey should be familiar to participants. In most AR4D resource allocation settings, what I have characterized above as "signals" should probably be referred to as "strategic objectives", or simply "goals".

In the hypothetical example presented here, the stakeholders have agreed that Economic Growth, Income Equality, Environmental Sustainability, and Nutritional Security are the dimensions that best describe the evolution of the AR4D proposals within the problem space that is of interest to them. The stakeholders' average asessment of each proposal's correlation with each of the strategic objectives is given in Figure \@ref(fig:loadsRotExamp).

During this exercise, participants should be encouraged to keep in mind that no AR4D proposal can "be all things to all people". A new yield enhancing variety of a high value crop, for example, might contribute towards increased trade competitiveness and GDP growth, but at the cost of increased deforestation and use of chemical inputs that degrade the environment. Conversely, a climate smart or pro-poor AR4D proposal might increase long term environmental and socio-economic sustainability at the cost of reduced short-medium term growth and competitiveness. These tradeoffs require careful consideration.^[Participants might also be encouraged to beware of any received wisdom regarding tradeoffs and synergies. For example, it has long been customary in AR4D communities to assume that economic growth and economic equality are mutually exclusive goals [@Alston1995], whereas recent empirical research suggests a much more nuanced and synergistic relation [@berg2017inequality].]

```{r loadsRotExamp, fig.show = "hold", fig.width = 6, fig.height = 4, fig.align = "center", fig.cap = "A hypothetical example of proposal-signal correlations (a.k.a. orthogonally rotated signal loadings) elicited from a stakeholder survey."}
df_x <- df_Lrot
df_x$Group <- NULL
prop_col <- df_x$Proposal
df_x$Proposal <- NULL
mat_Lrot <- as.matrix(df_x)
rownames(mat_Lrot) <- prop_col
list_groups <- list(prop_highYcommod, prop_smallHolder, prop_CSA)
# n_groups <- length(list_groups)
# bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
# group_colors <- sample(bag_of_colors, n_groups)
group_info <- list(list_groups, group_names, group_colors)

sigNames <- c("GDP Growth", "Economic\nEquality", "Environmental\nSustainability", "Nutritional\nSecurity")
xAxis_title <- "Rotated Correlations"
plot_corrXS_barchart(mat_Lrot, group_info, xAxis_title, sigNames)

```

These crowdsourced proposal-SO correlations are then interpreted as the orthogonally rotated loadings matrix $L_{\circlearrowleft}$. The signal covariance matrix ($\boldsymbol{\hat{\mu}}$) is then extracted from this information via equation \ref{eq:crowdsource_G}. The expected values of the signal proportionality parameters, meanwhile, are calculated from the elicited information via equations \ref{eq:deducePcrowd} and \ref{eq:sigMu_from_Xmu2}. The signal proportionality parameters and risk (i.e. the diagonal elements of $\tilde{\Gamma}$) are presented in Figure \ref{fig:sigRetRisk}. This is the information needed to conduct risk adjusted optimization over the signals portfolio. Before proceeding with that task, however, it may be worthwhile, as a stimulus to stakeholder discussion, to first deduce the approximate proposal covariance matrix from the crowdsourced information (Figure \ref{fig:covmatProps}).

```{r, fig.show = "hold", fig.width = 3, fig.height = 3, fig.align = "center", fig.cap = "\\label{fig:sigRetRisk}Expected AR4D signal proportionality parameters and risk derived from crowdsourced information."}

# Covariance matrix
D_sdX <- diag(10^2 * sdX_vec)
#D_sdX <- diag(1/sdX_vec)
mat_Q <- D_sdX %*% mat_Lrot
eig_decomp_QQ <- eigen(t(mat_Q) %*% mat_Q)
eig_values_QQ <- eig_decomp_QQ$values
mat_G <- diag(eig_values_QQ)
covmat_SS <- mat_G
```

```{r, fig.show = "hold", fig.width = 6, fig.height = 4, fig.align = "left", fig.cap = "\\label{fig:covmatProps}AR4D proposal covariance matrix derived from the elicited loadings."}

#D_sdX <- diag(10^2 * sdX_vec)
D_sdX <- diag(sdX_vec)
cormat_XX_derived <- mat_Lrot %*% t(mat_Lrot)
covmat_XX_derived <- D_sdX %*% cormat_XX_derived %*% D_sdX
covmat_XX_derived <- round(covmat_XX_derived, 4)
colnames(covmat_XX_derived) <- colnames(cormat_XX_derived)
fig_title <- "Crowdsourced AR4D Proposal Covariance Matrix"
plot_covmat(covmat_XX_derived, fig_title, round_to = 4, graph_on = F)

```

This matrix essentially answers the question "What is the synergy (or tradeoff) between the impacts of proposals $x$ and $y$?" Some of these answers may serve to confirm expectations. It probably comes as no surprise, for example, that the high yielding, high value AR4D proposals are strongly correlated with each other, or that the high value livestock AR4D proposal Ultra Cow is negatively correlated with the climate smart AR4D proposals. On the other hand, some of these correlations may suprise us, or serve to fill in a gap where there is simply no good information. A donor might ask, for example, "But how do heat tolerant beans fit in with digital agriculture?" At a minimum, this matrix provides a point of departure when confronted with such impromptu questions.



References